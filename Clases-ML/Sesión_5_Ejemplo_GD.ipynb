{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRJ4Y886XTTi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification, make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q = np.array([[4.0, 0.0],\n",
        "              [0.0, 1.0]])\n",
        "b = np.array([0.0, 0.0])  # tomar b=0 para simplificar; el mínimo está en 0\n",
        "\n",
        "def f(w):\n",
        "    return 0.5 * w @ Q @ w + b @ w\n",
        "\n",
        "def grad_f(w):\n",
        "    return Q @ w + b\n",
        "\n",
        "w0 = np.array([2.0, 2.0])\n",
        "etas = [0.2, 0.5, 1.2]  # probar estable, crítico-ish, e inestable\n",
        "steps = 30\n",
        "\n",
        "fig, ax = plt.subplots(1, len(etas), figsize=(14, 4))\n",
        "for j, eta in enumerate(etas):\n",
        "    w = w0.copy()\n",
        "    traj = [w.copy()]\n",
        "    vals = [f(w)]\n",
        "    diverged = False\n",
        "    for k in range(steps):\n",
        "        w = w - eta * grad_f(w)\n",
        "        traj.append(w.copy())\n",
        "        vals.append(f(w))\n",
        "        if np.linalg.norm(w) > 1e6:  # diverge guard\n",
        "            diverged = True\n",
        "            break\n",
        "    traj = np.array(traj)\n",
        "    # Contornos\n",
        "    xs = np.linspace(-2.5, 2.5, 200)\n",
        "    ys = np.linspace(-2.5, 2.5, 200)\n",
        "    X, Y = np.meshgrid(xs, ys)\n",
        "    Z = 0.5*(4*X**2 + 1*Y**2)\n",
        "    ax[j].contour(X, Y, Z, levels=10, linewidths=0.8, colors='gray')\n",
        "    ax[j].plot(traj[:,0], traj[:,1], '-o', ms=3)\n",
        "    ax[j].set_title(f\"GD en cuadrática (η={eta})\" + (\" (diverge)\" if diverged else \"\"))\n",
        "    ax[j].set_xlabel(\"$w_1$\")\n",
        "    ax[j].set_ylabel(\"$w_2$\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ep1mo77qayEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(n_samples=600, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_clusters_per_class=1, class_sep=1.5, random_state=0)\n",
        "y = 2*y - 1  # convertir a {-1, +1}\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0 + np.exp(-z))\n",
        "\n",
        "def logistic_loss_grad(w, X, y, lam=1e-2):\n",
        "    # f(w) = (1/n) sum log(1+exp(-y w^T x)) + lam/2 ||w||^2\n",
        "    n = X.shape[0]\n",
        "    z = y * (X @ w)\n",
        "    loss = np.mean(np.log(1 + np.exp(-z))) + 0.5*lam*np.dot(w, w)\n",
        "    # grad = -(1/n) sum [ y x * sig(-z) ] + lam w\n",
        "    g = -(X.T @ (y * (1 - sigmoid(z))))/n + lam*w\n",
        "    return loss, g\n",
        "\n",
        "w = np.zeros(X.shape[1])\n",
        "eta = 0.5\n",
        "epochs = 200\n",
        "history = []\n",
        "\n",
        "for t in range(epochs):\n",
        "    loss, g = logistic_loss_grad(w, X, y, lam=1e-2)\n",
        "    w = w - eta*g\n",
        "    history.append(loss)\n",
        "\n",
        "print(\"Loss final:\", history[-1])\n",
        "\n",
        "# Curva de entrenamiento\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(history)\n",
        "plt.xlabel(\"Iteración\")\n",
        "plt.ylabel(\"Función de pérdida\")\n",
        "plt.title(\"GD en regresión logística (L2=1e-2)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Frontera de decisión\n",
        "xx, yy = np.meshgrid(np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200),\n",
        "                     np.linspace(X[:,1].min()-1, X[:,1].max()+1, 200))\n",
        "Z = (np.c_[xx.ravel(), yy.ravel()] @ w).reshape(xx.shape)\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.contourf(xx, yy, (Z>0).astype(int), alpha=0.15, levels=1)\n",
        "plt.contour(xx, yy, Z, levels=[0], linewidths=2)\n",
        "plt.scatter(X[:,0], X[:,1], c=(y>0), s=15, cmap='bwr', edgecolor='k')\n",
        "plt.title(\"Frontera GD (logística)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1mmG2oeca39y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}