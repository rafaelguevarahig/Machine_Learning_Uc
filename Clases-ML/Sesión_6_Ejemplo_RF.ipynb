{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7-JfhqynYN4"
      },
      "outputs": [],
      "source": [
        "# === Celda A: RESET + PREPRO ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# --- Configuración de dataset/target ---\n",
        "PATH = \"/content/data.csv\"\n",
        "TARGET_COL = \"Loan_Status\"  # cámbialo si tu target es otro\n",
        "\n",
        "df = pd.read_csv(PATH)\n",
        "assert TARGET_COL in df.columns, f\"'{TARGET_COL}' no está en {df.columns.tolist()}\"\n",
        "\n",
        "# Quitar identificadores obvios si existen\n",
        "for c in [\"Loan_ID\",\"ID\",\"Id\",\"id\"]:\n",
        "    if c in df.columns:\n",
        "        df = df.drop(columns=[c])\n",
        "\n",
        "y = df[TARGET_COL].copy()\n",
        "X = df.drop(columns=[TARGET_COL]).copy()\n",
        "\n",
        "# Detectar tipos\n",
        "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Split (estratificado si es clasificación con pocas clases)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42,\n",
        "    stratify=y if y.nunique() <= 20 else None\n",
        ")\n",
        "\n",
        "# OneHotEncoder compatible con cualquier versión de scikit-learn\n",
        "try:\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # sklearn >= 1.2\n",
        "except TypeError:\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)         # sklearn < 1.2\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", ohe),\n",
        "])\n",
        "num_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"cat\", cat_pipe, cat_cols),\n",
        "    (\"num\", num_pipe, num_cols),\n",
        "], verbose_feature_names_out=False)\n",
        "\n",
        "# Fit/transform\n",
        "Xtr_proc = preprocess.fit_transform(X_train)\n",
        "Xte_proc = preprocess.transform(X_test)\n",
        "feature_names = preprocess.get_feature_names_out()\n",
        "\n",
        "print(\"Xtr_proc:\", Xtr_proc.shape, \"| Xte_proc:\", Xte_proc.shape)\n",
        "print(\"y_train:\", y_train.shape, \"| y_test:\", y_test.shape)\n",
        "print(\"Ejemplos de features:\", feature_names[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Celda B: MODELOS + MÉTRICAS ===\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "models = {}\n",
        "\n",
        "# Árbol interpretable\n",
        "tree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=5, random_state=42)\n",
        "tree.fit(Xtr_proc, y_train)\n",
        "models[\"DecisionTree(max_depth=4)\"] = tree\n",
        "\n",
        "# Random Forest (con OOB)\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300, max_depth=None, min_samples_leaf=2,\n",
        "    max_features=\"sqrt\", oob_score=True, n_jobs=-1, random_state=42\n",
        ")\n",
        "rf.fit(Xtr_proc, y_train)\n",
        "models[\"RandomForest\"] = rf\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingClassifier(\n",
        "    learning_rate=0.08, n_estimators=300, max_depth=3, random_state=42\n",
        ")\n",
        "gb.fit(Xtr_proc, y_train)\n",
        "models[\"GradientBoosting\"] = gb\n",
        "\n",
        "# (Opcional) XGBoost si está instalado\n",
        "xgb_ok = False\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    xgb_clf = xgb.XGBClassifier(\n",
        "        n_estimators=150, max_depth=3, learning_rate=0.1,\n",
        "        subsample=0.8, colsample_bytree=0.8,\n",
        "        reg_lambda=1.0, reg_alpha=0.0,\n",
        "        objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
        "        n_jobs=-1, random_state=42\n",
        "    )\n",
        "    xgb_clf.fit(Xtr_proc, y_train)\n",
        "    models[\"XGBoost\"] = xgb_clf\n",
        "    xgb_ok = True\n",
        "except Exception as e:\n",
        "    print(\"XGBoost no disponible:\", e)\n",
        "\n",
        "# Evaluación\n",
        "rows = []\n",
        "for name, clf in models.items():\n",
        "    ytr = clf.predict(Xtr_proc)\n",
        "    yte = clf.predict(Xte_proc)\n",
        "    row = {\n",
        "        \"modelo\": name,\n",
        "        \"acc_train\": accuracy_score(y_train, ytr),\n",
        "        \"acc_test\":  accuracy_score(y_test, yte)\n",
        "    }\n",
        "    if name == \"RandomForest\":\n",
        "        row[\"oob_score\"] = getattr(clf, \"oob_score_\", None)\n",
        "    rows.append(row)\n",
        "\n",
        "res_df = pd.DataFrame(rows).sort_values(\"acc_test\", ascending=False)\n",
        "print(res_df)\n"
      ],
      "metadata": {
        "id": "-gCK0-5KnoyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Celda C: Importancia de variables (impureza) ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_importance_impurity(clf, feat_names, title, top=15):\n",
        "    if not hasattr(clf, \"feature_importances_\"):\n",
        "        print(f\"{title}: el estimador no expone feature_importances_.\")\n",
        "        return None\n",
        "    imp = np.asarray(clf.feature_importances_)\n",
        "    idx = np.argsort(imp)[::-1][:top]\n",
        "    top_tbl = pd.DataFrame({\n",
        "        \"feature\": np.array(feat_names)[idx],\n",
        "        \"importance_impurity\": imp[idx]\n",
        "    })\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    y_pos = np.arange(len(idx))\n",
        "    plt.barh(y_pos, imp[idx])\n",
        "    plt.yticks(y_pos, np.array(feat_names)[idx])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return top_tbl\n",
        "\n",
        "imp_tables = {}\n",
        "if \"RandomForest\" in models:\n",
        "    imp_tables[\"RF\"] = plot_importance_impurity(models[\"RandomForest\"], feature_names, \"RandomForest — Importancia (impureza)\")\n",
        "if \"GradientBoosting\" in models:\n",
        "    imp_tables[\"GB\"] = plot_importance_impurity(models[\"GradientBoosting\"], feature_names, \"GradientBoosting — Importancia (impureza)\")\n",
        "\n",
        "# Mostrar top conjunto si existen ambos\n",
        "if imp_tables:\n",
        "    display({k: v.head(10) for k, v in imp_tables.items() if v is not None})\n"
      ],
      "metadata": {
        "id": "lI4dDBu4pIhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Celda D: Permutation Importance en test ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Elegimos mejor por acc_test de res_df\n",
        "assert \"res_df\" in globals(), \"No encuentro res_df; ejecuta Celda B primero.\"\n",
        "best_name = res_df.iloc[0][\"modelo\"]\n",
        "best_clf = models[best_name]\n",
        "print(\"Mejor modelo según acc_test:\", best_name)\n",
        "\n",
        "perm = permutation_importance(best_clf, Xte_proc, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
        "idx = np.argsort(perm.importances_mean)[::-1]\n",
        "top = 15\n",
        "idx = idx[:min(top, len(idx))]\n",
        "\n",
        "perm_tbl = pd.DataFrame({\n",
        "    \"feature\": np.array(feature_names)[idx],\n",
        "    \"perm_importance_mean\": perm.importances_mean[idx],\n",
        "    \"perm_importance_std\": perm.importances_std[idx]\n",
        "})\n",
        "display(perm_tbl)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "y_pos = np.arange(len(idx))\n",
        "plt.barh(y_pos, perm.importances_mean[idx])\n",
        "plt.yticks(y_pos, np.array(feature_names)[idx])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(f\"Permutation Importance — {best_name} (top {len(idx)})\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "06irQRArpPuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- elegir par de features como antes (si ya lo hiciste, puedes omitir esta parte) ---\n",
        "preferred = [\"ApplicantIncome\", \"Credit_History\"]\n",
        "use_feats = [f for f in preferred if f in feature_names]\n",
        "if len(use_feats) < 2:\n",
        "    try:\n",
        "        top2 = perm_tbl[\"feature\"].tolist()[:2]\n",
        "        use_feats = top2\n",
        "    except Exception:\n",
        "        # fallback: primeras dos columnas del espacio procesado\n",
        "        use_feats = feature_names[:2]\n",
        "\n",
        "print(\"Par de variables para el plano 2D:\", use_feats)\n",
        "\n",
        "mask = np.isin(feature_names, use_feats)\n",
        "Xtr_2d = Xtr_proc[:, mask]\n",
        "Xte_2d = Xte_proc[:, mask]\n",
        "\n",
        "# Asegurar y_train numérica para evitar sorpresas en clasificador\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "\n",
        "clf2d = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf2d.fit(Xtr_2d, y_train_enc)\n",
        "\n",
        "# Malla\n",
        "x_min, x_max = Xtr_2d[:,0].min()-0.3, Xtr_2d[:,0].max()+0.3\n",
        "y_min, y_max = Xtr_2d[:,1].min()-0.3, Xtr_2d[:,1].max()+0.3\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 350),\n",
        "                     np.linspace(y_min, y_max, 350))\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# Probabilidades para superficie continua\n",
        "proba = clf2d.predict_proba(grid)  # shape: (N, n_classes)\n",
        "if proba.shape[1] == 2:\n",
        "    # binario: tomamos proba de la clase positiva (etiqueta max del encoder)\n",
        "    Z = proba[:, 1].reshape(xx.shape)\n",
        "    levels = np.linspace(0.0, 1.0, 11)\n",
        "    cmap = \"RdBu\"\n",
        "else:\n",
        "    # multiclase: usamos el argmax de proba como superficie categórica (numérica)\n",
        "    Z = np.argmax(proba, axis=1).reshape(xx.shape).astype(float)\n",
        "    # niveles discretos (0..K-1)\n",
        "    n_classes = proba.shape[1]\n",
        "    levels = np.arange(-0.5, n_classes + 0.5, 1)\n",
        "    cmap = None  # dejar default\n",
        "\n",
        "plt.figure(figsize=(5.5, 4.5))\n",
        "cs = plt.contourf(xx, yy, Z, alpha=0.18, levels=levels, cmap=cmap)\n",
        "\n",
        "# Puntos de entrenamiento (colorear por la clase codificada)\n",
        "plt.scatter(Xtr_2d[:,0], Xtr_2d[:,1], c=y_train_enc, s=16, edgecolor=\"k\")\n",
        "plt.xlabel(use_feats[0]); plt.ylabel(use_feats[1])\n",
        "plt.title(f\"Frontera 2D — {use_feats[0]} vs {use_feats[1]} (probabilidades)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lXdtbAwlpPp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Celda G: GridSearch para RF y GB ===\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "rf_grid = {\n",
        "    \"n_estimators\": [200, 400],\n",
        "    \"max_features\": [\"sqrt\", 0.5],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "    \"max_depth\": [None, 8]\n",
        "}\n",
        "rf_gs = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42),\n",
        "    param_grid=rf_grid, scoring=\"accuracy\", cv=cv, n_jobs=-1\n",
        ")\n",
        "rf_gs.fit(Xtr_proc, y_train)\n",
        "print(\"RF best params:\", rf_gs.best_params_, \"| CV acc:\", rf_gs.best_score_)\n",
        "rf_best = rf_gs.best_estimator_\n",
        "print(\"RF OOB:\", getattr(rf_best, \"oob_score_\", None), \"| Test acc:\", rf_best.score(Xte_proc, y_test))\n",
        "\n",
        "gb_grid = {\n",
        "    \"n_estimators\": [200, 400],\n",
        "    \"learning_rate\": [0.05, 0.1],\n",
        "    \"max_depth\": [2, 3]\n",
        "}\n",
        "gb_gs = GridSearchCV(\n",
        "    estimator=GradientBoostingClassifier(random_state=42),\n",
        "    param_grid=gb_grid, scoring=\"accuracy\", cv=cv, n_jobs=-1\n",
        ")\n",
        "gb_gs.fit(Xtr_proc, y_train)\n",
        "print(\"GB best params:\", gb_gs.best_params_, \"| CV acc:\", gb_gs.best_score_)\n",
        "gb_best = gb_gs.best_estimator_\n",
        "print(\"GB Test acc:\", gb_best.score(Xte_proc, y_test))\n"
      ],
      "metadata": {
        "id": "V3u8Od8vpPbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Celda H: Reportes del mejor modelo ===\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "best_name = res_df.iloc[0][\"modelo\"]\n",
        "best_clf = models[best_name]\n",
        "y_pred = best_clf.predict(Xte_proc)\n",
        "\n",
        "print(\"Mejor modelo:\", best_name)\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClasification report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "SnbPtVSC6ueD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}